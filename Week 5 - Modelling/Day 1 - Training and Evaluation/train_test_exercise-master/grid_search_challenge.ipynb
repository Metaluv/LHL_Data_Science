{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perdiction of sales\n",
    "\n",
    "### Problem Statement\n",
    "The dataset represents sales data for 1559 products across 10 stores in different cities. Also, certain attributes of each product and store are available. The aim is to build a predictive model and find out the sales of each product at a particular store.\n",
    "\n",
    "|Variable|Description|\n",
    "|: ------------- |:-------------|\n",
    "|Item_Identifier|Unique product ID|\n",
    "|Item_Weight|Weight of product|\n",
    "|Item_Fat_Content|Whether the product is low fat or not|\n",
    "|Item_Visibility|The % of total display area of all products in a store allocated to the particular product|\n",
    "|Item_Type|The category to which the product belongs|\n",
    "|Item_MRP|Maximum Retail Price (list price) of the product|\n",
    "|Outlet_Identifier|Unique store ID|\n",
    "|Outlet_Establishment_Year|The year in which store was established|\n",
    "|Outlet_Size|The size of the store in terms of ground area covered|\n",
    "|Outlet_Location_Type|The type of city in which the store is located|\n",
    "|Outlet_Type|Whether the outlet is just a grocery store or some sort of supermarket|\n",
    "|Item_Outlet_Sales|Sales of the product in the particulat store. This is the outcome variable to be predicted.|\n",
    "\n",
    "Please note that the data may have missing values as some stores might not report all the data due to technical glitches. Hence, it will be required to treat them accordingly.\n",
    "\n",
    "\n",
    "\n",
    "### Explore the problem in following stages:\n",
    "\n",
    "1. Hypothesis Generation – understanding the problem better by brainstorming possible factors that can impact the outcome\n",
    "2. Data Exploration – looking at categorical and continuous feature summaries and making inferences about the data.\n",
    "3. Data Cleaning – imputing missing values in the data and checking for outliers\n",
    "4. Feature Engineering – modifying existing variables and creating new ones for analysis\n",
    "5. Model Building – making predictive models on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('regression_exercise.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3417caeb53134532aa7269e67b25879f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "533bafea676447f19aceb5e35183f72b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ba8a04d6668481eb9bbb83ae0d55378",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "661abbe3264a478e82dd445275bc867c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pandas_profiling import ProfileReport\n",
    "prof = ProfileReport(df)\n",
    "prof.to_file(output_file='output.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.copy()\n",
    "data['Item_Identifier_Type'] = data.Item_Identifier.astype(str).str[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['current_year'] = 2021\n",
    "data['Operation_Year'] = data.current_year - data.Outlet_Establishment_Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['Item_Fat_Content'] == 'LF', 'Item_Fat_Content'] = 'Low Fat'\n",
    "data.loc[data['Item_Fat_Content'] == 'low fat', 'Item_Fat_Content'] = 'Low Fat'\n",
    "data.loc[data['Item_Fat_Content'] == 'reg', 'Item_Fat_Content'] = 'Regular'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.replace({'Outlet_Location_Type': {'Tier 1': 1, 'Tier 2': 2, 'Tier 3': 3}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Outlet_Size\"] = data[\"Outlet_Size\"].fillna('Medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['Item_Identifier_Type'] == 'NC', 'Item_Fat_Content'] = 'None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Outlet_Size\"] = data[\"Outlet_Size\"].fillna('Medium')\n",
    "dropped = data[data.Item_Weight.notnull()]\n",
    "mean = dropped.groupby('Item_Type').Item_Weight.mean()\n",
    "name = pd.Series(dropped.Item_Type.unique())\n",
    "mean_weights = pd.concat([mean,name],axis=1,keys=['mean'])\n",
    "def fill_item(row):\n",
    "    if not np.isnan(row['Item_Weight']):\n",
    "        return row['Item_Weight']\n",
    "    else:\n",
    "        mean = mean_weights.loc[row.Item_Type, 'mean']\n",
    "        return mean\n",
    "data['Item_Weight'] = data.apply(fill_item, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data[[ 'Item_Weight', 'Item_Fat_Content', 'Item_Visibility', 'Item_MRP','Outlet_Size',\n",
    "       'Outlet_Type', 'Item_Outlet_Sales', 'Item_Identifier_Type', 'Operation_Year','Outlet_Location_Type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Item_Weight</th>\n",
       "      <th>Item_Fat_Content</th>\n",
       "      <th>Item_Visibility</th>\n",
       "      <th>Item_MRP</th>\n",
       "      <th>Outlet_Size</th>\n",
       "      <th>Outlet_Type</th>\n",
       "      <th>Item_Outlet_Sales</th>\n",
       "      <th>Item_Identifier_Type</th>\n",
       "      <th>Operation_Year</th>\n",
       "      <th>Outlet_Location_Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.300</td>\n",
       "      <td>Low Fat</td>\n",
       "      <td>0.016047</td>\n",
       "      <td>249.8092</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Supermarket Type1</td>\n",
       "      <td>3735.1380</td>\n",
       "      <td>FD</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.920</td>\n",
       "      <td>Regular</td>\n",
       "      <td>0.019278</td>\n",
       "      <td>48.2692</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Supermarket Type2</td>\n",
       "      <td>443.4228</td>\n",
       "      <td>DR</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17.500</td>\n",
       "      <td>Low Fat</td>\n",
       "      <td>0.016760</td>\n",
       "      <td>141.6180</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Supermarket Type1</td>\n",
       "      <td>2097.2700</td>\n",
       "      <td>FD</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19.200</td>\n",
       "      <td>Regular</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>182.0950</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Grocery Store</td>\n",
       "      <td>732.3800</td>\n",
       "      <td>FD</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.930</td>\n",
       "      <td>None</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>53.8614</td>\n",
       "      <td>High</td>\n",
       "      <td>Supermarket Type1</td>\n",
       "      <td>994.7052</td>\n",
       "      <td>NC</td>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8518</th>\n",
       "      <td>6.865</td>\n",
       "      <td>Low Fat</td>\n",
       "      <td>0.056783</td>\n",
       "      <td>214.5218</td>\n",
       "      <td>High</td>\n",
       "      <td>Supermarket Type1</td>\n",
       "      <td>2778.3834</td>\n",
       "      <td>FD</td>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8519</th>\n",
       "      <td>8.380</td>\n",
       "      <td>Regular</td>\n",
       "      <td>0.046982</td>\n",
       "      <td>108.1570</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Supermarket Type1</td>\n",
       "      <td>549.2850</td>\n",
       "      <td>FD</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8520</th>\n",
       "      <td>10.600</td>\n",
       "      <td>None</td>\n",
       "      <td>0.035186</td>\n",
       "      <td>85.1224</td>\n",
       "      <td>Small</td>\n",
       "      <td>Supermarket Type1</td>\n",
       "      <td>1193.1136</td>\n",
       "      <td>NC</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8521</th>\n",
       "      <td>7.210</td>\n",
       "      <td>Regular</td>\n",
       "      <td>0.145221</td>\n",
       "      <td>103.1332</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Supermarket Type2</td>\n",
       "      <td>1845.5976</td>\n",
       "      <td>FD</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8522</th>\n",
       "      <td>14.800</td>\n",
       "      <td>Low Fat</td>\n",
       "      <td>0.044878</td>\n",
       "      <td>75.4670</td>\n",
       "      <td>Small</td>\n",
       "      <td>Supermarket Type1</td>\n",
       "      <td>765.6700</td>\n",
       "      <td>DR</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8523 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Item_Weight Item_Fat_Content  Item_Visibility  Item_MRP Outlet_Size  \\\n",
       "0           9.300          Low Fat         0.016047  249.8092      Medium   \n",
       "1           5.920          Regular         0.019278   48.2692      Medium   \n",
       "2          17.500          Low Fat         0.016760  141.6180      Medium   \n",
       "3          19.200          Regular         0.000000  182.0950      Medium   \n",
       "4           8.930             None         0.000000   53.8614        High   \n",
       "...           ...              ...              ...       ...         ...   \n",
       "8518        6.865          Low Fat         0.056783  214.5218        High   \n",
       "8519        8.380          Regular         0.046982  108.1570      Medium   \n",
       "8520       10.600             None         0.035186   85.1224       Small   \n",
       "8521        7.210          Regular         0.145221  103.1332      Medium   \n",
       "8522       14.800          Low Fat         0.044878   75.4670       Small   \n",
       "\n",
       "            Outlet_Type  Item_Outlet_Sales Item_Identifier_Type  \\\n",
       "0     Supermarket Type1          3735.1380                   FD   \n",
       "1     Supermarket Type2           443.4228                   DR   \n",
       "2     Supermarket Type1          2097.2700                   FD   \n",
       "3         Grocery Store           732.3800                   FD   \n",
       "4     Supermarket Type1           994.7052                   NC   \n",
       "...                 ...                ...                  ...   \n",
       "8518  Supermarket Type1          2778.3834                   FD   \n",
       "8519  Supermarket Type1           549.2850                   FD   \n",
       "8520  Supermarket Type1          1193.1136                   NC   \n",
       "8521  Supermarket Type2          1845.5976                   FD   \n",
       "8522  Supermarket Type1           765.6700                   DR   \n",
       "\n",
       "      Operation_Year  Outlet_Location_Type  \n",
       "0                 22                     1  \n",
       "1                 12                     3  \n",
       "2                 22                     1  \n",
       "3                 23                     3  \n",
       "4                 34                     3  \n",
       "...              ...                   ...  \n",
       "8518              34                     3  \n",
       "8519              19                     2  \n",
       "8520              17                     2  \n",
       "8521              12                     3  \n",
       "8522              24                     1  \n",
       "\n",
       "[8523 rows x 10 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have covered data preparation and feature engineering two weeks ago. Now, it's time to do some predictive models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "\n",
    "## Task\n",
    "Make a baseline model. Baseline model is the one which requires no predictive model and its like an informed guess. For instance, predict the sales as the overall average sales or just zero.\n",
    "Making baseline models helps in setting a benchmark. If your predictive algorithm is below this, there is something going seriously wrong and you should check your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures \n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-76b3c652581e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mreg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mreg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "reg = LinearRegression()\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "r2_score(y_test, y_pred)\n",
    "#mean_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R^2:\t0.6092251236999888\n",
      "Test R^2:\t0.5979246262761988\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "764.6755205865103"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xpoly_train = PolynomialFeatures(degree=2).fit_transform(X_train)\n",
    "Xpoly_test = PolynomialFeatures(degree=2).fit_transform(X_test)\n",
    "\n",
    "reg.fit(Xpoly_train, y_train)\n",
    "ypoly_train_pred = reg.predict(Xpoly_train)\n",
    "ypoly_test_pred = reg.predict(Xpoly_test)\n",
    "\n",
    "r2poly_train = r2_score(y_train, ypoly_train_pred)\n",
    "r2poly_test = r2_score(y_test, ypoly_test_pred)\n",
    "print(f'Train R^2:\\t{r2poly_train}\\nTest R^2:\\t{r2poly_test}')\n",
    "mean_absolute_error(y_test, ypoly_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "Split your data in 80% train set and 20% test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data2.Item_Outlet_Sales\n",
    "X = data2.drop(['Item_Outlet_Sales'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = [cname for cname in X_train.columns if X_train[cname].nunique() < 10 and \n",
    "                        X_train[cname].dtype == \"object\"]\n",
    "\n",
    "numerical_cols = [cname for cname in X_train.columns if X_train[cname].dtype in ['int64', 'float64']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Item_Weight',\n",
       " 'Item_Visibility',\n",
       " 'Item_MRP',\n",
       " 'Operation_Year',\n",
       " 'Outlet_Location_Type']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "# categorical_transformer= OneHotEncoder(handle_unknown='ignore')\n",
    "# numerical_transformer=StandardScaler()\n",
    "\n",
    "ct = make_column_transformer(\n",
    "    (StandardScaler(), numerical_cols), #turn all values in these columns between 0 and 1 \n",
    "    (OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    ")\n",
    "ct.fit(X_train)\n",
    "\n",
    "#transform training and test data with normalization (MinMaxScaler) and OneHOtEncoder\n",
    "X_train = ct.transform(X_train)\n",
    "X_test = ct.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "Use grid_search to find the best value of parameter `alpha` for Ridge and Lasso regressions from `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 28 candidates, totalling 140 fits\n",
      "The best hyperparameter settings achieve a cross-validated R^2 of: 0.5637254605986994\n",
      "Alpha:\t0.1\n",
      "L1 ratio:\t1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jesse\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.5635729  0.56363144 0.56368249 0.56371567 0.56370714        nan\n",
      "        nan 0.56142116 0.56208927 0.56267165 0.56321737 0.56370796\n",
      "        nan        nan 0.51573713 0.52890277 0.54266837 0.55565177\n",
      " 0.56372546        nan        nan 0.3118371  0.34642315 0.39084482\n",
      " 0.454348   0.56358063        nan        nan]\n",
      "  warnings.warn(\n",
      "C:\\Users\\jesse\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 215611147.1047697, tolerance: 1961030.3060812112\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import ElasticNet\n",
    "# Make a dictionary with model arguments as keys and lists of grid settings as values\n",
    "param_grid = {\n",
    "    'alpha': [0.001, 0.01, 0.1, 1],\n",
    "    'l1_ratio': [0, 0.25, 0.5, 0.75, 1,1.25,5]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(estimator=ElasticNet(), param_grid=param_grid, cv=5, scoring='r2', verbose=1, n_jobs=-1) # verbose=1 -> print results, n_jobs=-1 -> use all processors in parallel\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "best_r2 = grid_result.best_score_\n",
    "best_alpha = grid_result.best_params_['alpha']\n",
    "best_l1_ratio = grid_result.best_params_['l1_ratio']\n",
    "print(f'The best hyperparameter settings achieve a cross-validated R^2 of: {best_r2}\\nAlpha:\\t{best_alpha}\\nL1 ratio:\\t{best_l1_ratio}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jesse\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:530: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 215611147.1047697, tolerance: 1961030.3060812112\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "858.149993214268"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elas = ElasticNet(alpha=0.1, l1_ratio = 1)\n",
    "elas.fit(X_train, y_train)\n",
    "y_pred =  elas.predict(X_test)\n",
    "mean_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "Using the model from grid_search, predict the values in the test set and compare against the benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-1.4.2-py3-none-win_amd64.whl (97.8 MB)\n",
      "Requirement already satisfied: numpy in c:\\users\\jesse\\anaconda3\\lib\\site-packages (from xgboost) (1.19.5)\n",
      "Requirement already satisfied: scipy in c:\\users\\jesse\\anaconda3\\lib\\site-packages (from xgboost) (1.6.2)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.4.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\"learning_rate\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,\n",
    " \"max_depth\"        : [ 3, 4, 5, 6, 8, 10, 12, 15],\n",
    " \"min_child_weight\" : [ 1, 3, 5, 7 ],\n",
    " \"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n",
    " \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline MAE is 1366.92\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# \"Learn\" the mean from the training data\n",
    "mean_train = np.mean(y_train)\n",
    "# Get predictions on the test set\n",
    "baseline_predictions = np.ones(y_test.shape) * mean_train\n",
    "# Compute MAE\n",
    "mae_baseline = mean_absolute_error(y_test, baseline_predictions)\n",
    "print(\"Baseline MAE is {:.2f}\".format(mae_baseline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "             importance_type='gain', interaction_constraints='',\n",
       "             learning_rate=0.2, max_delta_step=0, max_depth=15,\n",
       "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "             n_estimators=1000, n_jobs=-1, num_parallel_tree=1, random_state=0,\n",
       "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "             tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "my_model = XGBRegressor(n_estimators=1000, learning_rate=0.2, n_jobs=-1,max_depth = 15)\n",
    "my_model.fit(X_train, y_train, \n",
    "             early_stopping_rounds=5, \n",
    "             eval_set=[(X_test, y_test)], \n",
    "             verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "829.1464939487528"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = my_model.predict(X_test)\n",
    "r2_score(y_test, y_pred)\n",
    "mean_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 192 candidates, totalling 960 fits\n",
      "The best hyperparameter settings achieve a cross-validated R^2 of: 0.5965073125716851\n",
      "Alpha:\t0.0\n",
      "L1 ratio:\t0.4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import ElasticNet\n",
    "# Make a dictionary with model arguments as keys and lists of grid settings as values\n",
    "param_grid = {\"learning_rate\"    : [0.05, 0.10, 0.15 ] ,\n",
    " \"max_depth\"        : [ 3, 4, 5, 6],\n",
    " \"min_child_weight\" : [ 1, 3, 5, 7 ],\n",
    " \"gamma\"            : [ 0.0, 0.1],\n",
    " \"colsample_bytree\" : [ 0.3, 0.4 ] }\n",
    "\n",
    "grid = GridSearchCV(estimator=XGBRegressor(), param_grid=param_grid, cv=5, scoring='r2', verbose=1, n_jobs=-1) # verbose=1 -> print results, n_jobs=-1 -> use all processors in parallel\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "best_r2 = grid_result.best_score_\n",
    "best_alpha = grid_result.best_params_['gamma']\n",
    "best_l1_ratio = grid_result.best_params_['colsample_bytree']\n",
    "print(f'The best hyperparameter settings achieve a cross-validated R^2 of: {best_r2}\\nAlpha:\\t{best_alpha}\\nL1 ratio:\\t{best_l1_ratio}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
